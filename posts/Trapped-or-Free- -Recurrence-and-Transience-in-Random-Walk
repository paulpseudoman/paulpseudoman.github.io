---
authors:
- Aritrabha Majumdar
date: "2024-10-02"
math: true
title: Apple Bread Challenges
---
### Basic Setup:
We consider the line of integers, namely $\mathbf{Z}$. Now, we consider a bug is walking on this line. Say, as an example, the bug starts at the origin, and moves to 1, then to 2, then to 3, then to 2, then to 1, then to 2 again, then to 1, then to origin, then to $-1$, then to origin again. We take each stationary time increment as 1.

Now, we would like to look into a more formal definition of the {mess} hapenning. We fix $N \in\mathbb{N}$, and we say the {configuration space} is given by the following binary sequence of length $N$
\[
\Omega_N = \left\{\omega = (\omega_1,\dots,\omega_N)\in\{-1,+1\}^N\right\}
\]
Now we write
\[
X_k(\omega) = \omega_k
\]
To denote the position of the random walk at time $k$. The position of the random walk after $n$ steps (or after time $n$) is
\[
S_n(\omega) = \sum_{k = 1}^{n}X_k(\omega), \hspace{0.25cm}1\leq n\leq N,\hspace{0.25cm} S_0(\omega) = 0
\]
Thus $\forall\omega\in\Omega_N$, we obtain a trajectory $(S_n)_{n = 0}^N$, which we call a {path}. As a probability distribution on $\Omega_N$, we take the uniform distribution, i.e.
\[
\mathbb{P}(A) = \frac{|A|}{2^N},\hspace{0.25cm}A\subseteq\Omega_N
\]
Now we give a formal definition.
**Definition:** The sequence of rankdom variables $(S_n)_{n = 0}^N$ on finite probability space $(\Omega_N, \mathbb{P})$ is known as a {Simple Random Walk of length $N$ starting at 0}. Note that it has independent increments. Even, we can have a strongr condition, that is, $X_m \coprod X_n$, $m,n\in\mathbb{N}$.

### Distribution of $S_n$ and Probability of returning to the origin
We want to find out $\mathbb{P}(S_n = x)$. So we write
\begin{align*}
    & X_n \sim \text{Reidemester}\left(\frac{1}{2}\right)\\
    & \implies \frac{X_n + 1}{2} \sim \text{Ber}\left(\frac{1}{2}\right)\\
    & \implies \frac{S_n + n}{2} \sim \text{Bin}\left(n,\frac{1}{2}\right)\\
    & \implies \mathbb{P}(S_n = x) = \frac{\binom{2n}{\frac{n+x}{2}}}{2^n}
\end{align*}
Observe that the distribution is symmetric around origin. Moreover, the maximal probability is achieved at the {middle}, or when $x=0$ and $n$ is even. We have 
\[
\mathbb{P}(S_{2n} = 0) = \binom{2n}{n} 2^{-2n}
\]
Now we apply Stirling's Approximation $n! = \left(\frac{n}{e}\right)^n\sqrt{2\pi n}$ for large $n$, and obtain
\[
\mathbb{P}(S_{2n} = 0)\sim\frac{1}{\sqrt{\pi n}}
\]
Now, is it really a coincidence that we obtain the mode when the random walk returns to the origin, or doees it have deeper significance? We will observe them at the following sections.

### Random Walk in Higher Dimensions:
Now we extend the formal definition of teh random walk on $\mathbb{Z}$ to random walk on $\mathbb{z}^d$, $d\in\mathbb{N}$.\\
We fix $d\in\mathbb{N}$. For $x\in\mathbb{Z}^d$, we write
\[
||x|| = \sqrt{\left(\sum_{j = 1}^{d}x_j^2\right)}
\]
For given $N\in \mathbb{N}$, we have
\[
\Omega_{N} = \left\{\omega = (\omega_1, \dots, \omega_N)\mid\omega_k\in\mathbb{Z}^d, |\omega_k| = 1, \forall 1\leq n\leq N\right\}
\]
In the similar fashion as we did before, we can define $X_k(\omega) = \omega_k$, $1\leq k\leq N$, and 
\[
S_n(\omega) = \sum_{k = 1}^{n}X_k(\omega),\hspace{0.25cm} 1\leq n \leq N, \hspace{0.25cm} S_0(\omega) = 0.
\]
We stil have an {uniform distribution}; i,e for $A\subseteq \Omega_N$, $\mathbb{P}(A) = \frac{|A|}{(2d)^{-N}}$. We also observe that $S_n$ in this case is a $d$ dimensional random vector.
\[
S_n = \begin{pmatrix}
    S_n^{(1)}\\
    \vdots\\
    S_N^{(d)}
\end{pmatrix}, \hspace{0.25cm} 0\leq n \leq N,\hspace{0.25cm} S_n^{(j)}\in\mathbb{Z}, j = 1,2,\dots,d
\]
Now, George Polya showed that for random walk in $\mathbb{Z^d}$, We have \[\mathbb{P}(S_n = 0) = \Theta\left(n^{-\frac{d}{2}}\right)\]
Which holds when $d = 1$.

### Recurrence and Transience

Let $a\in\mathbb{N}$ and we define $\sigma_a = \min\{n\in\mathbb{N}\mid S_n = a\}$ is the first hitting time of $a$ after time $0$. We define a {random walk} is {recurrent} if $\mathbb{P}(\sigma_0 < \infty) = 1$. Otherwise, it is {transient}.

### An Important Result:
 We start by claiming
 \[\mathbb{P}(S_n = 0) = \sum_{i = 1}^n\mathbb{P}(\sigma_0 = i)\mathbb{P}(S_{n-i} = 0)\hspace{0.25cm},\hspace{0.25cm}n\in\mathbb{N}
 \]
Now we take $z\in[0,1]$ and consider two generating functions
\[ G(0,z) = \sum_{n = 0}^{\infty}z^n\mathbb{P}(S_n = 0)\hspace{0.25cm}F(0,z) = \sum_{n = 0}^{\infty}z^n\mathbb{P}(\sigma_{0} = n)
\] 
The motivation of the function $G$ is obtained from {Random Walk Green's Function}. This is defined by 
\[
G(x,1) = \sum_{n = 0}^{\infty}\mathbb{P}(S_n = x) = \sum_{n = 0}^{\infty}\mathbb{E}\left(\mathds{1}_{\{S_n = x\}}\right) = \mathbb{E}\left(\sum_{n = 0}^{\infty}\mathds{1}_{S_n = x}\right)
\]
So it gives us expected number of visits to $x$. Observe $\{\sigma_0 = 0\} = \phi$ and $\mathbb{P}(S_0 = 0) = 1$. Now
    \begin{align*}
        G(0,z) &= 1 + \sum_{n\in\mathbb{N}}z^n\mathbb{P}(S_n = 0) = 1 + \sum_{n\in\mathbb{N}}\sum_{i = 1}^{n}z^i\mathbb{P}(\sigma_0 = i)z^{n-i}\mathbb{P}(S_{n-i} = 0)\\
         &= 1 + \sum_{i \in \mathbb{N}}z^i\mathbb{P}(\sigma_0 = i)\sum_{j \in \mathbb{N}_0}z^j\mathbb{P}(S_j = 0) = 1 + F(0,z)G(0,z)
    \end{align*}
    This can be rewritten as $F(0,z) = 1 - {G(0,z)}^{-1}$.
     Now we have some interesting observations.
     \[ \sum_{n = 1}^{N}\mathbb{P}(\sigma_0 = n) = F(0,1) = \lim_{z \uparrow 1} F(0,z) = 1 - \lim_{z\uparrow1}\frac{1}{G(0,z)}\]
     If we have
     \[
     G(0,1) = \sum_{n = 0}^{\infty} \mathbb{P}(S_n = 0) < \infty
     \]
Then clearly $F(0,1) < \infty$, i.e the walk is {transient}.
On the other hand, if
\[
     G(0,1) = \sum_{n = 0}^{\infty} \mathbb{P}(S_n = 0) = \infty
     \]
Then we fix $\varepsilon > 0$ and $N_\varepsilon \in \mathbb{N}$. such that $$ \sum_{n = 0}^{N} \mathbb{P}(S_n = 0) \geq \frac{2}{\varepsilon} $$ Then for $z$ sufficiently close to 1, we have $z^n \geq \frac{1}{2}$, which gives $$ \sum_{n = 0}^{N} z^n\mathbb{P}(S_n = 0) \geq \frac{1}{\varepsilon} $$. So we have \[
\frac{1}{G(0,z)} \leq \frac{1}{\sum_{n = 0}^{N} z^n\mathbb{P}(S_n = 0)} \leq \varepsilon
\] Since our $\varepsilon$ is arbitray, we have \[
\lim_{z\uparrow1}G(0,z)^{-1} = 0
\]
So, $F(0,1) = 0$, and the walk is {recurrent}.

### Recurrence in 1D:
As we have already seen, 
\[
\mathbb{P}[S_{2n} = 0] \sim \frac{1}{\sqrt{\pi n}} 
\]
Now
\[
    \sum_{n = 0}^{\infty} \frac{1}{\sqrt{\pi n}} = \infty
\]
Hence, the random walk is indeed {recurrent}.
### Recurrence in 2D: 
We could have done this in the {traditional} way we have already seen, but here we introduce even {sleeker} proof for 2D. Let us consider a {corner walk}, in which each move adds with equal probability one of $\{(1,1), (-1,-1), (1,-1), (-1,1)\}$ to the current location. The advantage of this walk is it is independent in each coordinate and they resemble simple random walk in $\mathbb{Z}$. So
\[\mathbb{P}\{X_{2t} = (0,0)\} = \mathbb{P}_{(0,0)}\left\{ X_{2t}^{(1)}  \right\} \mathbb{P}_{(0,0)}\left\{ X_{2t}^{(2)}\right\}\sim \frac{c}{n}
\]
Now, the random walk in $\mathbb{Z}^2$ is the 45 degrees rotation of the {corner walk}. Now, if we consider $\mathbf{Y} = A \mathbf{X}$, then jacobian of the inverse transform has determinant 1 , and it is pretty easy to observe that $Y_{2t}^{(1)}$ and $Y_{2t}^{(2)}$ are indeed independent. As we know $\sum_{n = 1}^{\infty}\frac{1}{n} = \infty$, the random walk is recurrent.\\

In general, what we have is 
\[
\sum_{n = 0}^\infty \mathbb{P}(S_n = 0) = \sum_{n = 0}^{\infty}\frac{c}{n^{\frac{d}{2}}} 
\]
This sum is divergent iff $d = 1, 2$, i.e the random walk is {recurrent} when $d = 1,2$ and transient when $d \geq 3$. Below, we present a more rigorous  proof of this, which does not involve Polya'} Recurrence Theorem.

\subsection{Approaching the Problem using Fourier Analysis}
We know that a finite measure $\mu$ on $\mathbb{Z}^{d}$ uniquely determines its characteristic function

$$
\phi_{\mu}(k)=\mathbb{E}\left(e^{i k x}\right)=\sum_{x \in \mathbb{Z}^{d}} e^{i k \cdot x} \mu(\{x\}),\hspace{0.25cm}x\in\mathbb{Z}^d
$$

Here $k \cdot x=\sum_{i=1}^{d} k_{i} x_{i}$ is the standard inner product.
Now, we are willing to retrieve $\mu\left(\{x\}\right)$. So we use Fourier Inversion Formula

$$
\mu(\{x\})=\frac{1}{(2 \pi)^{d}} \int_{[-\pi, \pi)^{d}} e^{-i k \cdot x} \phi_{\mu}(k) d k
$$

Now, we already know that

$$
\mu(x)=\frac{1}{2 d} \mathds{1}_{\{|x|=1\}}, \quad x \in \mathbb{Z}^{d}
$$

Via symmetry: we derive an useful representation of $\phi_{\mu}(k)$

$$
\phi_{\mu}(k)=\frac{1}{d} \sum_{j=1}^{d} \cos \left(k_{j}\right)
$$

As $X_{1}, \ldots, X_{n}$ are iid random variables; we apply The convolution rule of characteristic function and obtain

$$
\mathbb{P}\left(S_{n}=x\right)=\mathbb{P}\left(\sum_{i=1}^n X_{i}=x\right)=\frac{1}{(2 \pi)^{d}} \int_{[-\pi, \pi)^{d}} e^{-i k \cdot x}(\phi(k))^{n} d k
$$

$\therefore$ For $z \in[0,1]$; we have

$$
\begin{aligned}
G(0, z) & =\sum_{n=0}^{\infty} z^{n} \mathbb{P}\left(S_{n}=0\right)=\sum_{n=0}^{\infty} \frac{1}{(2 \pi)^{d}} \int_{[-\pi, \pi)^{d}} z^{n}(\phi(k))^{n} d k \\
& =\frac{1}{(2 \pi)^{d}} \int_{[-\pi, \pi)^{d}} \frac{d k}{1-z \phi(k)}
\end{aligned}
$$

As $\lim _{z \uparrow 1} G(0, z)=G(0,1) ;$ Hereby we com say

$$
G(0,1)<\infty \Longleftrightarrow \int_{(-\pi, \pi)^d} \frac{d k}{1-\phi(k)}<\infty
$$

Now, we use a bound for $(1-\cos{k})$ ts obtain our desired result.

$$
\begin{aligned}
& \frac{2 k_{j}^{2}}{\pi^{2}} \leqslant 1-\cos k_{j} \leqslant \frac{k_{i}^{2}}{2^{2}} \\
\implies & \sum_{j=1}^{d} \frac{2 k_{j}^{2}}{\pi^{2}} \leqslant d-\sum_{j=1}^{d} \cos k_{j} \leqslant \sum_{j=1}^{d} \frac{k_{j}^{2}}{2} \\
\implies & \frac{2}{d x^{2}} \sum_{j=1}^{d} k_{j}^{2} \leqslant 1-\phi(k) \leqslant \frac{1}{2 d} \sum_{j=1}^{d} \frac{k^{2}}{2} k_{j}^{2}
\end{aligned}
$$

$$
\begin{aligned}
& \implies \frac{d \pi^{2}}{2} \frac{1}{\|k\|^{2}} \geqslant \frac{1}{1-\phi(k)} \geqslant 2 d \frac{1}{\|k\|^{2}} \\
& \implies \frac{d A^{2}}{2} \int_{[-\pi, \pi]^{d}} \frac{d k}{\|k\|^{2}} \geqslant \int_{[-\pi, \pi)^{d}} \frac{d(k}{1-\phi(k)} \geqslant 2 d \int_{[-\pi, \pi)^{d}} \frac{d k}{\|k\|^{2}}
\end{aligned}
$$

Now, $f(k)=\frac{1}{\|k\|^{2}}$ is a function on $\mathbb{R}^{+}$ and $f: \mathbb{R}^{+} \to[0, \infty)$. So, we can write

$$
\begin{aligned}
& \int_{[-\pi, \pi) d} \frac{d k}{\|k\|^{2}}=2 \operatorname{Vol}\left(S^{d-1}\right) \int_{0}^{\pi} \frac{1}{r^{2}} r^{d-1} d r \\
&=2 \operatorname{Vol}\left(S^{d-1}\right)_{0}^{\pi} \int_{0}^{\pi} r^{d-3} d r \\
& \therefore \int_{[\pi, \pi) d} \frac{d k}{1-\phi(k)} \leqslant d \operatorname{Vol}\left(s^{d-1}\right) \pi^{2} \int_{0}^{\pi} r^{d-3} d r
\end{aligned}
$$

This is enough to claim that $G(0,1)<\infty \iff
 d \geqslant 3$\\
On the other hand; we use.

$$
\int_{[-\pi, \pi)^{d}} \frac{d k}{1-\phi(k)} \geqslant 4 d \operatorname{Vol}\left(S^{d-1}\right) \int_{0}^{\pi} r^{d-3} d r
$$

To claim $G(0,1)=\infty \iff
 d=1,2$
### Conclusion
As we explored today, recurrence demonstrates how some systems, no matter how random, have a certain inevitability in their behavior, while transience highlights the unpredictable journey where some paths are taken once and never again. These concepts extend far beyond mathematicsâ€”they represent a way of thinking about dynamics, movement, and the probability of return in many real-world systems. So to conclude this talk, i just want to restate a famous quote by Shizuo Kakutani

***A drunk man will find his way home, but a drunk bird may get lost forever.***